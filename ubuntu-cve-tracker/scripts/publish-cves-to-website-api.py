#! /usr/bin/env python3
#
# SPDX-License-Identifier: GPL-3.0-only 
#
# This file is part of the Ubuntu CVE Tracker (UCT) scripts, its purpose is to
# publish CVEs to web API
#
# Author: Mark Morlino <mark.morlino@canonical.com>
# Author: Mike Salvatore <mike.salvatore@canonical.com>
# Copyright 2020 Canonical Ltd.
#
# This program is free software: you can redistribute it and/or modify it under
# the terms of the GNU General Public License version 3, as published by the
# Free Software Foundation.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranties of MERCHANTABILITY,
# SATISFACTORY QUALITY, or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License along with
# this program. If not, see <http://www.gnu.org/licenses/>.

# Standard library
import os
import random
import sys
import time

import requests

import cve_lib
import re
import argparse
import json
from http.cookiejar import MozillaCookieJar

# Local - macaroonbakery is needed to interact with LP
from macaroonbakery import httpbakery

CVE_FILE_REGEX = "CVE-\\d{4}-\\d{4,}$"

IGNORE_CACHE = os.path.join(os.path.expanduser("~"), ".publish-cves-ignore-cache")

# set the number of CVEs to update in one submission to the api endpoint
CVE_UPDATE_ENDPOINT = "cves.json"
DEFAULT_CHUNKSIZE = os.getenv("WEB_API_CHUNKSIZE", 50)


def authentication(url):
    """
    Authenticate with Macaroons in order to use Webteam API. macaroonbakery knows how to deal with LP.

    Once that is done, it can be switched to a regular Session to avoid rebuilding the connection every time.
    """
    try:
        client = httpbakery.Client(
            cookies=MozillaCookieJar(
                os.path.join(os.path.expanduser("~"), ".ubuntu.com.login")
            )
        )

        if os.path.exists(client.cookies.filename):
            client.cookies.load(ignore_discard=True)

        response = client.request("PUT", url=url, json=[])
        response.raise_for_status()
        client.cookies.save(ignore_discard=True)

        session = requests.Session()
        session.cookies = client.cookies

        return session
    except requests.exceptions.HTTPError as err:
        print("Authentication failed. Reason: %s" % err, file=sys.stderr)
        print(err.response.text.strip(), file=sys.stderr)
        sys.exit(1)


def get_codename(raw_codename, cve_releases):
    codename = raw_codename.split("/")[0]

    if codename != "devel":
        return codename

    return get_devel_codename(cve_releases)


def get_tags(cve_data):
    tags = {}
    for pkg in cve_data["tags"]:
        # Skip the global tags key since the web API doesn't handle this
        # currently
        if pkg == cve_lib.GLOBAL_TAGS_KEY:
            continue
        tags[pkg] = []
        for tag in cve_data["tags"][pkg]:
            tags[pkg].append(tag)
    return tags


def get_patches(cve_data):
    patches = {}
    for pkg in cve_data["patches"]:
        temp = []
        for source, entry in cve_data["patches"].get(pkg):
            # NOTE: KLUDGE1: cve_lib is not aware of patch notes in entry
            entry_match = re.match(r"(\S+) \((.+)\)", entry)
            if entry_match:
                url = entry_match.groups()[0]
                # NOTE: KLUDGE2: external web tooling cannot handle notes
                #                drop them.
                #       https://github.com/canonical/ubuntu.com/issues/13498
                # note = entry_match.groups()[1]
            else:
                url = entry
            temp.append(f"{source}: {url}")
        patches[pkg] = temp
    return patches


def get_devel_codename(cve_releases):
    for skip_release in ["upstream", "devel", "product", "snap"]:
        if skip_release in cve_releases:
            cve_releases.remove(skip_release)

    if len(cve_releases) <= 0:
        print("WARNING: No valid ubuntu releases in CVE", file=sys.stderr)
        return None

    cve_releases = cve_lib.release_sort(cve_releases)

    devel_release_index = cve_lib.releases.index(cve_releases[-1]) + 1
    if devel_release_index >= len(cve_lib.releases) or devel_release_index < 0:
        print(
            "WARNING: Could not determine devel release codename. Perhaps it hasn't "
            "been added to cve_lib.all_releases yet?",
            file=sys.stderr,
        )
        return None

    cve_devel_release = cve_lib.releases[devel_release_index]

    return cve_devel_release


def process_cve(cve_filename):
    # Upload active and ignored (in Ubuntu)
    cve_data = cve_lib.load_cve(cve_filename)

    references = cve_data["References"].split("\n")
    if references[0] == "":
        references.pop(0)

    cvss3 = None
    impact = None
    if len(cve_data["CVSS"]) > 0:
        if "3." in cve_data["CVSS"][0]["vector"]:
            # Use CVSS3
            try:
                cvss3_data = cve_lib.parse_cvss(cve_data["CVSS"][0]["vector"])
                cvss3_data["baseMetric"]["cvss"]["baseScore"] = float(
                    cve_data["CVSS"][0]["baseScore"]
                )
                cvss3_data["baseMetric"]["cvss"]["baseSeverity"] = cve_data["CVSS"][0][
                    "baseSeverity"
                ]
                impact = {
                    "baseMetricV3": {"cvssV3": {**cvss3_data["baseMetric"]["cvss"]}}
                }
                cvss3 = cvss3_data["baseMetric"]["cvss"]["baseScore"]
            except ValueError as e:
                print(
                    "%s: bad CVSS data %s, skipping: %s"
                    % (cve_filename, cve_data["CVSS"][0]["vector"], e),
                    file=sys.stderr,
                )
                cvss3 = None
                impact = None

    packages = []
    tags = get_tags(cve_data)
    patches = get_patches(cve_data)
    for pkg in cve_data["pkgs"]:
        statuses = []
        cve_releases = cve_data["pkgs"][pkg].keys()
        cve_releases = [rel for rel in cve_releases if rel in cve_lib.releases]

        for codename in cve_lib.releases + ["upstream"]:
            status = None
            pocket = "security"

            # Set the public release first
            if codename in cve_data["pkgs"][pkg]:
                status = cve_data["pkgs"][pkg][codename]

            # Release could be missing, but have a esm/subproject release
            if (
                not status or status[0] != "released"
            ) and codename in cve_lib.get_active_releases_with_subprojects(
                include_esm=True
            ):
                # Check for possible product statuses
                for release in [
                    codename + "/esm",
                    "esm-infra/" + codename,
                    "esm-infra-legacy/" + codename,
                    "esm-apps/" + codename,
                    "ros-esm/" + codename,
                    "ros-esm/" + codename + "/foxy",
                    "ros-esm/" + codename + "/noetic",
                    "fips/" + codename,
                    "fips-updates/" + codename,
                    "realtime/" + codename,
                ]:
                    if release in cve_data["pkgs"][pkg]:
                        status_overwrite = cve_data["pkgs"][pkg][release]

                        # for any esm/subproject:
                        # if 'released' we want pocket and status
                        if status_overwrite[0] == "released":
                            pocket = (
                                "esm-infra"
                                if release == "trusty/esm"
                                else release.split("/")[0]
                            )
                            status = status_overwrite
                            break

                        # for active subprojects:
                        # if not 'released', we want the pocket and status
                        product, series = cve_lib.product_series(release)
                        if product in cve_lib.get_active_subprojects_by_release(series):
                            pocket = product
                            status = status_overwrite

                        # for EOL releases:
                        # if not 'released', we want status (pocket is 'security')
                        if codename in cve_lib.eol_releases:
                            status = status_overwrite

            if status:
                statuses.append(
                    {
                        "release_codename": codename,
                        "status": status[0],
                        "description": status[1],
                        "pocket": pocket,
                    }
                )
        package = {
            "name": pkg,
            "source": f"https://launchpad.net/ubuntu/+source/{pkg}",
            "ubuntu": f"https://packages.ubuntu.com/search?suite=all&section=all&arch=any&searchon=sourcenames&keywords={pkg}",
            "debian": f"https://tracker.debian.org/pkg/{pkg}",
            "statuses": statuses,
        }
        packages.append(package)

    status = "active"

    if "** REJECT **" in cve_data["Description"]:
        status = "rejected"

    notes = []

    # TODO Remove this when we have the proper field is ready
    if len(cve_data["Priority"]) > 1 and cve_data["Priority"][1]:
        notes.append(
            {"author": "", "note": "Priority reason:\n" + cve_data["Priority"][1]}
        )

    for [author, note] in cve_data["Notes"]:
        notes.append({"author": author, "note": note})

    priority = cve_data["Priority"][0]

    if priority == "untriaged":
        priority = "unknown"

    cve = {
        "id": cve_data["Candidate"],
        "description": cve_data["Description"],
        "ubuntu_description": cve_data["Ubuntu-Description"],
        # plenty of CVEs in retired/ do not have a Mitigation section at all
        "mitigation": cve_data.get("Mitigation", ""),
        "notes": notes,
        "priority": priority,
        "cvss3": cvss3,  # CVSS3 computed base score
        "references": references,
        "bugs": cve_data["Bugs"].strip().split("\n"),
        "packages": packages,
        "status": status,
        "tags": tags,
        "patches": patches,
    }

    if impact:
        cve["impact"] = impact  # Full CVSS3 base vector structure

    if cve_data["PublicDate"] != "unknown":
        cve["published"] = cve_data["PublicDate"]

    return cve


def load_ignore_cache():
    ignore_cache = set()
    with open(IGNORE_CACHE) as ic:
        for cve in ic:
            ignore_cache.add(cve.strip())

    return ignore_cache


def add_cve_to_ignore_cache(cve_id):
    with open(IGNORE_CACHE, "at") as ic:
        ic.write(cve_id + "\n")


def report_cves_in_collection(cve_collection, outfile=sys.stdout):
    # turn the full cve structures into just a dict of CVEs with a list
    # of packages as the elements, to limit the amount of output when
    # reporting errors
    simple_cves = dict()

    for cve_entry in cve_collection:
        packages = []
        if "packages" in cve_entry:
            for package in cve_entry["packages"]:
                packages.append(package["name"])
        simple_cves[cve_entry["id"]] = packages
    print(json.dumps(simple_cves, indent=2), file=outfile)


def req_size(req):
    return len(req.body) if req.body else 0


def delete_single_cve(args, cve):
    endpoint = f"{args.endpoint}cves/{cve}.json"
    if args.dry_run:
        print(f"DEBUG: would send delete http request for: {endpoint}")
    else:
        if args.verbose:
            print(f"INFO: sending delete http request for: {endpoint}")
        try:
            response = args.session.delete(endpoint)
            response.raise_for_status()
        except requests.exceptions.HTTPError as err:
            print(err.response.status_code, err.response.text)
            if args.stop:
                sys.exit(1)


def delete_cves(args, cve_list):
    pattern = re.compile(CVE_FILE_REGEX)
    for cve in cve_list:
        if not pattern.match(cve):
            print(f"WARNING: skipping deletion of non-CVE '{cve}'", file=sys.stderr)
            continue
        delete_single_cve(args, cve)


def push_chunks(args, chunk, retry=0):
    if args.verbose:
        print(f"INFO: pushing chunk: {chunk}")
        print(json.dumps(chunk, indent=2))
    try:
        response = args.session.put(f"{args.endpoint}{CVE_UPDATE_ENDPOINT}", json=chunk)
        response.raise_for_status()
        print(response.status_code, response.text.strip())
    except requests.exceptions.HTTPError as err:
        print(err.response.status_code, err.response.text.strip(), file=sys.stderr)
        # Edge case - 413 Content Too Large
        # Can happen if we bundle a group of large CVEs
        # Switch to pushing individual entries for this chunk
        if err.response.status_code == 413:
            return push_individual_entries(args, chunk)

        if retry <= 5:
            print(f"Sleeping {5 * (retry + 1)} seconds before retrying chunk...")
            time.sleep(5 * (retry + 1))
            return push_chunks(args, chunk, retry + 1)

        if args.stop:
            print(
                "PUSH FAILED [%s %s size %d bytes: %d %s]"
                % (
                    err.response.request.method,
                    err.response.request.url,
                    req_size(err.response.request),
                    err.response.status_code,
                    err.response.reason,
                ),
                file=sys.stderr,
            )
            report_cves_in_collection(chunk, outfile=sys.stderr)
            sys.exit(1)
        else:  # After failing 5 times, it's sent back to be picked up in the next loop
            print("PUT failed for bulk update, adding chunk to the retry queue")
            return chunk


def push_individual_entries(args, chunk, retry=0):
    for cve in chunk:
        if args.verbose:
            print(json.dumps(cve, indent=2))
        try:
            response = args.session.put(
                f"{args.endpoint}{CVE_UPDATE_ENDPOINT}", json=[cve]
            )
            response.raise_for_status()
            print(response.status_code, response.text.strip())
        except requests.exceptions.HTTPError as err:
            print(err.response.status_code, err.response.text.strip(), file=sys.stderr)
            if retry <= 5:
                print("Retrying individual entries in chunk...")
                return push_individual_entries(args, chunk, retry + 1)

            if args.stop:
                print(
                    "PUSH FAILED [%s %s size %d bytes: %d %s]"
                    % (
                        err.response.request.method,
                        err.response.request.url,
                        req_size(err.response.request),
                        err.response.status_code,
                        err.response.reason,
                    ),
                    file=sys.stderr,
                )
                report_cves_in_collection(chunk, outfile=sys.stderr)
                sys.exit(1)
            else:
                print(
                    "PUT failed for individual entries, adding chunk to the retry queue"
                )
                return chunk


def main(args):
    # Check for deletion operation
    if args.delete is not None:
        delete_cves(args, args.delete)

    args.session = authentication(f"{args.endpoint}{CVE_UPDATE_ENDPOINT}")

    cve_sync_list = []

    cve_filename_pattern = re.compile(
        f".*/?{CVE_FILE_REGEX}" if args.filename_check else ".*"
    )
    not_for_us_filename_pattern = re.compile(".*/not-for-us.txt$")
    ignore_paths = ["experimental", "subprojects", "scripts"]
    cache_not_for_us_cve_ids = []

    for cve_filename in args.file_path:
        if os.path.isdir(cve_filename):  # Sync the full directory
            cve_files = []
            # Note os.listdir gives unsorted list depending on filesystem
            for file in os.listdir(cve_filename):
                if args.verbose:
                    print(file)
                if cve_filename_pattern.match(file):
                    cve_files.append(file)

            cve_files = sorted(cve_files)
            print(f"Processing {len(cve_files)} in '{cve_filename}' directory")

            for filename in cve_files:
                relative_path = f"{cve_filename}/{filename}"
                cve = process_cve(relative_path)
                cve_sync_list.append(cve)

        elif not_for_us_filename_pattern.match(cve_filename):
            ignore_cache = load_ignore_cache()

            not_for_us_cve_ids = cve_lib.parse_CVEs_from_uri(cve_filename)
            print(
                f"Processing {len(not_for_us_cve_ids)} from '{cve_filename}' as not for us"
            )

            cache_not_for_us_cve_ids = [
                cve_id for cve_id in not_for_us_cve_ids if cve_id not in ignore_cache
            ]
            print(
                f"{len(cache_not_for_us_cve_ids)} not-for-us CVEs have not yet been processed"
            )

            for cve_id in cache_not_for_us_cve_ids:
                cve_sync_list.append(
                    {
                        "id": cve_id,
                        "notes": [
                            {
                                "author": "ubuntu-security",
                                "note": "Does not apply to software found in Ubuntu.",
                            }
                        ],
                        "references": [f"https://www.cve.org/CVERecord?id={cve_id}"],
                        "status": "not-in-ubuntu",
                    }
                )

        elif any(x in cve_filename for x in ignore_paths):
            print(f"skipping {cve_filename}")
            continue

        elif cve_filename_pattern.match(cve_filename) and os.path.isfile(cve_filename):
            cve = process_cve(cve_filename)
            cve_sync_list.append(cve)

        else:
            print(f"'{cve_filename}' is not a CVE file. Skipping...")

    print(f"{len(cve_sync_list)} total CVEs")

    random.shuffle(cve_sync_list)

    # Split into batches
    chunksize = args.chunksize

    print(f"Chunking updates in batches of {chunksize}")

    if args.dry_run:
        print("Dry run complete")
    else:
        retry_counter = (
            1_000  # Keeps looping over the list until every chunk has been updated
        )
        remaining = len(cve_sync_list)

        while retry_counter > 0:
            retry_counter -= 1

            retry_list = []
            try:
                for chunk in [
                    cve_sync_list[i : i + chunksize]
                    for i in range(0, len(cve_sync_list), chunksize)  # noqa: E203
                ]:
                    retry_chunk = push_chunks(args, chunk)
                    if retry_chunk is not None:
                        retry_list.extend(retry_chunk)
                    else:
                        remaining -= chunksize
                        print("Remaining", remaining)

                if len(retry_list) == 0:
                    print("Job completed")
                    break
                else:
                    print(f"Retrying {len(retry_list)} chunks")
                    cve_sync_list = retry_list
            except requests.exceptions.ConnectionError as err:
                # The connection is sometimes forcefully reset, refresh the session and carry on
                print("Connection reset. Creating a fresh session and continuing...")
                time.sleep(15)
                args.session = authentication(f"{args.endpoint}{CVE_UPDATE_ENDPOINT}")
                continue

        for cve_id in cache_not_for_us_cve_ids:
            add_cve_to_ignore_cache(cve_id)

    return cve_sync_list


def get_argparser():
    parser = argparse.ArgumentParser(
        description="This file loads CVEs to webteam's db, using the endpoint ubuntu.com/security/updates",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument(
        "-n",
        "--dry-run",
        action="store_true",
        default=False,
        help="Simulate, don't actually push to webteams db.",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        default=False,
        help="Print details of data sent to the webteams db.",
    )
    parser.add_argument(
        "--stop",
        action="store_true",
        help="Exit after non-200 status.",
    )
    parser.add_argument(
        "--chunksize",
        action="store",
        type=int,
        help="Number of CVEs to submit in each block",
        default=DEFAULT_CHUNKSIZE,
    )
    parser.add_argument(
        "--ignore-filename-check",
        action="store_false",
        help="Ignore if the file name isn't in the CVE-YYYY-NNNNN format",
        dest="filename_check",
        default=True,
    )
    parser.add_argument(
        "--endpoint",
        action="store",
        type=str,
        default="https://ubuntu.com/security/updates/",
        help="API endpoint url.",
    )
    parser.add_argument(
        "--delete",
        action="append",
        type=str,
        help="CVE(s) to delete from the web endpoint",
    )
    parser.add_argument(
        "file_path",
        action="store",
        type=str,
        nargs="*",
        help="[Required] The path of the CVE file(s) or folder(s)",
    )

    return parser


if __name__ == "__main__":
    parser = get_argparser()

    main(parser.parse_args())

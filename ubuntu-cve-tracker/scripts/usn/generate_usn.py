# -*- coding: utf-8 -*-
# Module containing functions for generating an Ubuntu Security Notice (USN)
#
# Author: Nick Galanis <nick.galanis@canonical.com>
# Copyright (C) 2025 Canonical Ltd.
#
# This script is distributed under the terms and conditions of the GNU General
# Public License, "Version" 3 or later. See http://www.gnu.org/copyleft/gpl.html
# for details.
#
import argparse
import json
import os
import yaml
from collections import defaultdict
from datetime import datetime, timezone
from utils import usn_number, packages, new_usn_dir, download_and_parse_changes_file, UCT, print_success, print_error, print_info, remove_empty_fields

def generate_usn(packages, ppas, wanted_releases):
    # Define file paths for input and output
    template_file = os.path.join(new_usn_dir, f"USN-{usn_number}-template.yaml")
    output_file = os.path.join(new_usn_dir, f"USN-{usn_number}.json")

    with open(template_file, "r") as file:
        data_dict = yaml.safe_load(file)

    # open the edited file with the minimum binaries
    minimum_binaries_file = os.path.join(new_usn_dir, f"minimum_binaries-{usn_number}.yaml")

    with open(minimum_binaries_file, "r") as file:
        min_binaries_data = yaml.safe_load(file)

    # open the metadata file containing the package descriptions 
    desc_file = os.path.join(UCT, 'meta_lists/package_info_overrides.json')
    with open(desc_file, "r") as file:
        pkg_info_overrides = json.load(file)

    filtered_binaries = {}
    cves = set([])
    # get the packages and the binaries dict from the helper function
    specified_releases = (wanted_releases != None)

    with open(os.path.join(new_usn_dir, 'pkgs_dict.json'), 'r') as pkgs_file:
        pkgs = json.load(pkgs_file)
    with open(os.path.join(new_usn_dir, 'binaries_dict.json'), 'r') as binaries_file:
        binaries = json.load(binaries_file)

    print_success("USN binaries loaded")
    # construct the releases dictionary

    releases_dict = defaultdict(lambda: {
        'allbinaries': {},
        'binaries': {},
        'sources': {},
        'archs': defaultdict(lambda: {
            'urls': {}
        })
    })


    for package, releases in pkgs.items():
        for release in releases:
            if specified_releases and release not in wanted_releases:
                continue
            print_info(f"Constructing binary links for {release}...")

            # get the allbinaries info from the generated dict
            releases_dict[release]['allbinaries'].update(binaries[package][release])

            # construct the archs (links) element and find the CVEs present in the USN
            # We do not want the archs field if the package is published in an esm pocket
            is_esm = any("esm-" in binary["pocket"] for binary in binaries[package][release].values())
            # read from the generated pkgs dict, get each source package and the releases inside of it
            if release in releases:
                archs = {}
                for arch in releases[release]:
                    # process binary links, only for non-esm releases
                    if arch != "source" and (not is_esm):
                        # for all the archs, changes file contains all the binary links that we want
                        changes_file = releases[release][arch].get("changes")
                        binaries_dict = releases[release][arch].get("binaries")
                        archs = download_and_parse_changes_file(changes_file, release, binaries_dict, cves, is_source=False, is_esm=is_esm)
                        # check if we have an "all" arch
                        all_arch = any("_all.deb" in binary for binary in binaries_dict.values())
                        if all_arch:
                            # if yes, we will keep track of the all binaries separately
                            for in_arch_key, in_arch_value in archs.items():
                                if "_all.deb" in in_arch_key:
                                    if hasattr(releases_dict[release]['archs']['all']['urls'], in_arch_key):
                                        releases_dict[release]['archs']['all']['urls'][in_arch_key].update(in_arch_value)
                                    else:
                                        releases_dict[release]['archs']['all']['urls'][in_arch_key] = in_arch_value
                                else:
                                    if hasattr(releases_dict[release]['archs'][arch]['urls'], in_arch_key):
                                        releases_dict[release]['archs'][arch]['urls'][in_arch_key].update(in_arch_value)
                                    else:
                                        releases_dict[release]['archs'][arch]['urls'][in_arch_key] = in_arch_value
                        else:
                            releases_dict[release]['archs'][arch]['urls'].update(archs)
                    # for the source, the changes file contains the .dsc and .debian.tar.xz files info
                    else:
                        changes_file = releases[release][arch].get("changes")
                        # and the dsc file contains the .orig files info
                        dsc_file = releases[release][arch].get("dsc")
                        sources_dict = releases[release][arch]
                        # we need to parse the _source.changes file for esm as well to get the CVEs
                        archs = download_and_parse_changes_file(changes_file, release, sources_dict, cves, is_source=True, is_esm=is_esm)
                        # process the orig file to add the links, only for a non-esm release
                        if not is_esm:
                            archs_orig = download_and_parse_changes_file(dsc_file, release, sources_dict, cves, is_source=True, is_esm=is_esm)
                            releases_dict[release]['archs'][arch]['urls'].update(archs)
                            releases_dict[release]['archs'][arch]['urls'].update(archs_orig)
            # construct the binaries element
            if release in min_binaries_data:
                # get the entries in min binaries for this release
                yaml_entries = set(min_binaries_data[release])
                # get the entries from allbinaries that appear in the min_binaries yaml file
                filtered_binaries = {
                    # keep all the keys except of the "source"
                    key: {k: v for k, v in value.items() if k != "source"}
                    for key, value in releases_dict[release]['allbinaries'].items()
                    if key in yaml_entries
                }
            releases_dict[release]['binaries'].update(filtered_binaries)

            # construct the sources element
            # read from the generated pkgs dict, get each source package and the releases inside of it
            if release in releases:
                # get its version from the dict
                version = releases[release]["source"].get("version", "unknown")
                # construct the sources dict
                if package in pkg_info_overrides:
                    description = pkg_info_overrides[package]["description"]
                else:
                    description = "XXX Change description"
                    print_error(f"WARNING: No entry found in package_info for {package}, please change the upstream name in the packages metadata")
                releases_dict[release]['sources'][package] = {
                    # get the source package's description from the metadata file
                    "description": description,
                    "version": version
                }
            print_success(f"Links for {release} constructed!")

    # finally, assign the releases dict in the correct place in the final USN
    data_dict['releases'] = releases_dict
    # add the timestamp to the USN
    data_dict['timestamp'] = datetime.now(timezone.utc).timestamp()
    # add the CVEs to the USN
    data_dict['cves'] = list(cves)
    # Everything else is pre-loaded from the yaml file, so save the dictionary to the USN JSON file
    with open(output_file, "w") as file:
        json.dump(remove_empty_fields(data_dict), file, indent=4, sort_keys=True)
    print_success(f"Final USN JSON file saved in {output_file}")

if __name__ == "__main__":

    parser = argparse.ArgumentParser(description="Generate a new USN.")
    parser.add_argument("--ppas", help="comma separated list of ESM PPAs that will be present in the USN. (optional, only for publishing to ESM)", default=None)
    parser.add_argument("--releases", help="comma separated list of supported Ubuntu releases that will be presented in the USN.", default=None)


    args = parser.parse_args()
    if args.ppas:
        ppas = args.ppas.split(',')
    else:
        ppas = None
    if args.releases:
        releases = args.releases.split(',')
    else:
        releases = None

    generate_usn(packages, ppas, releases)

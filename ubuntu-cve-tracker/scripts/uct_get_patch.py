#!/usr/bin/env python3
"""
    Module to download patch files from UCT CVE entries
"""

from collections import defaultdict
from pathlib import Path
from typing import Any
import re
import os
import logging
import argparse
import sys
from apt_pkg import version_compare

import requests

from cve_lib import get_cve_list, load_cve

logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)
log.setLevel(logging.INFO)


def get_download_link(link: str) -> tuple[str, str | None]:
    """
    Given patch link transforms it into a downloadable link

    Args:
        link: patch link to transform

    Returns:
        tuple[str, str | None]: Downloadable patch link and any associated tag
    """

    # TODO: Needs significantly more cases to be handled
    #       and better heuristics

    tag = None
    original_link = link

    if " " in link:
        link, tag = link.split(" ", maxsplit=1)

    if ("github" in link) or ("gitlab" in link) or ("salsa" in link):
        if not link.endswith(".patch"):
            link = link + ".patch"
    elif "savannah" in link:
        link = link.replace("commit", "patch")
    elif re.search(r"a=tree", link):
        link = re.sub(r"a=[^;]+", "a=patch", link)
        link = re.sub(r";h=[^;]+", "", link)
    elif "gitweb" in link and not re.search(r"a=[^;]+", link):
        # If gitweb link doesn't have format specified set it to patch
        link = link + ";a=patch"
    else:
        link = re.sub(r"a=[^;]+", "a=patch", link)

    if link == original_link:
        print("This link wasn't transformed!")
        print("Please implement in the get_download_link method")

    return link, tag


def is_merge_commit(link: str, patch_file: str) -> bool:
    """
    Determines if a patch_file/link is/refers to a merge commit

    Args:
        link(str): Patch file link
        patch_file (str): Contents of the patch
    """
    # TODO: Improve heuristic to detect pull request links
    multiple_patch_re = re.compile(r"PATCH [0-9]+/[0-9]+")

    return (
        ("pull" in link)
        or ("merge_requests" in link)
        or (multiple_patch_re.search(patch_file) is not None)
    )


def split_merge_commit(merge_commit: str) -> list[str]:
    """
    Splits a merge commit into separate commits

    Args:
        merge_commit (str): merge commit to split

    Returns:
        list[str]: list of commits
    """
    commit_pattern = re.compile("^From [a-f0-9]{40} ")
    patch_num = len(re.findall(commit_pattern, merge_commit))

    if patch_num == 0:
        log.error("No commits detected in merge_commit")
        return [merge_commit]

    patches = []
    current_patch = ""

    for line in merge_commit.splitlines(True):
        if commit_pattern.match(line) and current_patch:
            patches.append(current_patch)
            current_patch = ""

        current_patch += line

    patches.append(current_patch)

    return patches


def check_rate_limit(patch_file: str) -> bool:
    """
    Checks if you have been rate limited depending on the contents received from your request

    Args:
        patch_file (str): content to parse for rate limit messages

    Returns:
        bool: Returns true if evidence of rate limiting is present otherwise false
    """

    # TODO: Improve heuristic for rate limit detection

    return r"<h1>Too many requests</h1>" in patch_file


def handle_auth_tokens() -> dict[str, str]:
    """
    Returns:
        dict[str, str]: Auth headers needed to prevent rate limiting
    """

    # Attempt to get GitHub auth token to prevent rate limiting
    github_token = os.getenv("GITHUB_TOKEN", None)

    headers = {}

    if github_token:
        log.debug("Set Authorization header to $GITHUB_TOKEN")
        headers["Authorization"] = f"token {github_token}"

    return headers


def get_uct_path(uct_path_str: str) -> Path | None:
    """
    Converts uct_path_str to a Path object and checks that the relevant dirs exist

    Args:
        uct_path_str (str): path to UCT repository

    Returns:
        (Path | None): Returns UCT path as Path object if it doesn't exist then None
    """
    if not uct_path_str:
        uct_path_str = os.getenv("UCT", "")

    if not uct_path_str:
        log.error("UCT path not specified and environment variable not set!")
        log.error("Cannot download patches until path or environment variable is set")
        return None

    uct_path = Path(uct_path_str)

    # Inform user if no embargoed dir found
    if not (uct_path / "embargoed").is_dir():
        log.info("No embargoed directory found in UCT")

    # Check UCT path and active CVE dirs exist
    if (not uct_path.is_dir()) or (not (uct_path / "active").is_dir()):
        log.error("%s or active dirs don't exist", str(uct_path))
        return None

    return uct_path


def get_cves(cve_ids: list[str], pkg: str, uct_path: Path) -> list[dict[str, Any]]:
    """
    Given list of cve_ids parses relevant UCT entries
    Args:
        cve_ids (list[str]): list of CVE ids, if list is empty search all cves matching package
        pkg (str): Used for filtering CVEs when all_cves is set
        uct_path (Path): Path to UCT repository

    Returns:
        list[dict[str, Any]]: List of UCT CVE files parsed as a dictionary
    """

    # If no cve_ids supplied search all CVEs
    all_cves = not cve_ids

    if all_cves:
        print(f"Getting all CVEs for {pkg}, this will take a while...\n")

    cves = []
    all_cve_ids, embargoed_cve_ids = get_cve_list()

    for cve_id in all_cve_ids if all_cves else cve_ids:
        cve_path = (
            uct_path
            / ("embargoed" if cve_id in embargoed_cve_ids else "active")
            / cve_id
        )

        try:
            cve = load_cve(str(cve_path.resolve()))
            # We need to filter by package if using all CVEs
            if all_cves:
                if pkg in cve["pkgs"].keys():
                    cves.append(cve)
            else:
                cves.append(cve)
        except ValueError:
            log.critical("Issue with %s", cve_id)
        except FileNotFoundError:
            log.info("Could not find %s (possibly embargoed)", cve_id)

    return cves


def get_version_from_tag(tag: str) -> str | None:
    """
    Detect version within tag and return it otherwise None
    """

    version_re = re.compile(r"([0-9]+\.)+[0-9x]+")
    version_match = version_re.search(tag)
    version_str = version_match.group(0) if version_match else None

    # In the case that the tag contains an "x" to denote a range of versions
    # We set "x" to "999" so that all versions of that range will be picked
    # TODO: Implement better
    if version_str and "x" in version_str:
        version_str = version_str.replace("x", "999")

    return version_str


def get_tag_match(release: str, release_version: str, tags: list[str]) -> str:
    """
    Given a list of tags return the best suited one
    for the given release

    Args:
        release (str): Ubuntu release
        release_version (str): Version of package
        tags (list[str]): Tags to pick from
    """

    # TODO: Implement better

    # For all tags that can be parsed as versions create a map
    # from the versions back to the tags
    version_tag_map: dict[str, str] = {get_version_from_tag(tag): tag for tag in tags if (get_version_from_tag(tag) is not None)}  # type: ignore

    # Order versions ascending
    ascending_versions: list[str] = sorted(
        version_tag_map.keys(), key=lambda x: [int(y) for y in x.split(".")]
    )

    # Parses Epoch, Upstream Version and Revision
    version_parts_re = re.compile(
        r"^(?:([0-9]+):)?((?:(?:[0-9]+\.)+[0-9]+)(?:\+dfsg[^-]*)?)(?:-(.*))?$"
    )
    version_parts_match = version_parts_re.search(release_version)

    if not version_parts_match:
        # Nothing to do
        return tags[0]

    #epoch = version_parts_match.group(1)
    upstream_version = version_parts_match.group(2)
    #revision = version_parts_match.group(3)

    if not upstream_version:
        log.error("Could not parse upstream version from %s", release_version)
        return tags[0]

    # Find the lowest version that is higher than the given version
    # this is the appropriate tag to use for this release
    for version in ascending_versions:
        # If this version is higher than or equal to the release
        # version use its associated tag
        if version_compare(version, upstream_version) >= 0:
            return version_tag_map[version]

    # If we can't match a version check if the "None" tag exists
    if "None" in tags:
        return "None"

    # TODO: Better info/error messages
    log.debug(f"Couldn't map {release_version} to any of the tags: {tags}")
    return tags[0]


def get_uct_patch(
    cve_ids: list[str],
    pkg: str,
    patch_dir: Path,
    release_version_map: dict[str, str],
    uct_path_str: str = "",
    force: bool = False,
    verbose: bool = False,
    timeout: int = 10,
) -> dict[str, int]:
    """
    Gets patch files from either the PATCH_DB or links present in
    the UCT entry if possible

    Args:
        cves (list[str]): list of CVES to search for patch files, if list is empty assume
                            we want all cves for the specified package
        pkg (str): limit patches to package (use when multiple packages for the same CVE)
        dest (str): file path to store found patch files (default: current path)
        uct_path_str (str): path to uct repository
        force (bool): Will download request contents even in case of rate limiting
        silent (bool): If set silences output

    Returns:
        dict[str, int]: Maps cve_id to the number of patches downloaded for it
    """
    if verbose:
        # Disable all output
        log.setLevel(logging.DEBUG)

    uct_path = get_uct_path(uct_path_str)

    if uct_path is None:
        return {}

    cves = get_cves(cve_ids, pkg, uct_path)

    if not cves:
        log.error("No CVEs found or supplied")
        return {}

    if not patch_dir.is_dir():
        log.critical(
            "Destination %s doesn't exist or isn't a directory", patch_dir.name
        )
        return {}

    return {
        cve["Candidate"]: download_cve_patches(
            pkg, patch_dir, release_version_map, force, cve, timeout
        )
        for cve in cves
    }

def invalid_tag(tag: str) -> bool:
    skip_keywords: list[str] = [
        "wrong",
        "incomplete"
    ]

    return any((kw in tag) for kw in skip_keywords)


def download_cve_patches(
    pkg: str,
    patch_dir: Path,
    release_version_map: dict[str, str],
    force: bool,
    cve: dict[str, Any],
    timeout: int = 10,
) -> int:
    cve_id = cve["Candidate"]
    print(f"\033[96mGetting patches for {cve_id}\033[00m")

    headers = handle_auth_tokens()

    # Maps tag to a list of releases it applies to
    tag_release_map: dict[str, list[str]] = defaultdict(list[str])

    # For each tag, create a mapping from it to the
    # patches associated with it. This will be used to create a mapping
    # between the patches and ubuntu releases
    tag_patch_map: dict[str, list[str]] = defaultdict(list[str])

    # If patches exist for this pkg/cve pair
    if ("patches" in cve) and (pkg in cve["patches"]) and len(cve["patches"][pkg]):
        # NOTE: discarded part corresponds to "upstream", "debian", etc.
        for type, patch_link in cve["patches"][pkg]:
            if type != "upstream":
                print(f"Skipping ({type}: {patch_link})")
                continue

            download_link, tag = get_download_link(patch_link)

            if not tag:
                # Use this for links with no tag
                tag = "None"

            # Determines whether patch link should be used or ignored due to tag
            if invalid_tag(tag):
                print(f"\033[93mSkipping {download_link} because of tag \"{tag}\"\033[00m")
                continue

            try:
                patch_file = requests.get(
                    download_link, headers=headers, timeout=timeout
                ).text
                print(f"Downloaded patch from {download_link}")

                if check_rate_limit(patch_file):
                    log.error(
                        'You have been rate limited when trying to download "%s"',
                        download_link,
                    )
                    log.error("Skipping patch")
                    log.error(
                        "In the case of GitHub or GitLab patch links please"
                        "add env variables to reduce rate limits"
                    )

                    # TODO: Detect github/gitlab links and link to how to setup auth token
                    if not force:
                        continue

                if is_merge_commit(download_link, patch_file):
                    start_len = len(tag_patch_map[tag])
                    tag_patch_map[tag].extend(split_merge_commit(patch_file))
                    log.info(
                        "Merge commit detected (Patches %s-%s)",
                        start_len,
                        len(tag_patch_map[tag]),
                    )
                else:
                    tag_patch_map[tag].append(patch_file)
            except requests.exceptions.Timeout:
                log.error("Timed out (%ss)downloading %s patch from %s", timeout, cve_id, download_link)
            except requests.exceptions.RequestException:
                log.error("Failed to download %s patch from %s", cve_id, download_link)

        # If tag_patch_map is empty then every link failed to download (only case where
        # tag_patch_map doesn't have any mappings)
        if not tag_patch_map:
            print(f"\033[91mNo patches downloaded for {cve_id}\033[00m\n")
            return 0


        # Iterate each Ubuntu release and map it to a tag
        for release in release_version_map:
            tag = get_tag_match(
                release, release_version_map[release], list(tag_patch_map.keys())
            )
            tag_release_map[tag].append(release)

        log.debug("Tag Release Mapping:\n%s", tag_release_map)

        # For each tag store their associated patches in either all the releases
        # related to that tag or store it in the top level in the case of the
        # "None" tag
        for tag, patches in tag_patch_map.items():
            num_patches = len(patches)

            for index, patch in enumerate(patches):
                patch_filename = (
                    cve_id if num_patches == 1 else f"{cve_id}-{index + 1}"
                ) + ".patch"

                # Paths to save patch to
                patch_paths: list[Path] = []

                # If tag isn't "None" then we need to put the patches
                # in the release directories it applies to. Otherwise it
                # should go in the top level of the patches directory
                if tag != "None":
                    for release in tag_release_map[tag]:
                        patch_paths.append(
                            patch_dir / release.replace("/", "_") / patch_filename
                        )
                else:
                    patch_paths.append(patch_dir / patch_filename)

                for patch_path in patch_paths:
                    with open(patch_path, "w", encoding="utf-8") as f:
                        f.write(patch)

        # Sum all patches downloaded for each tag only if there is at least one release
        # that used the tag
        npd = sum(map(len, (tag_patch_map[tag] for tag in tag_release_map if tag_release_map[tag])))
        print(f"\033[92m{npd} Patches saved for {cve_id}\033[00m\n")
        return npd

    print(f"\033[93m{cve_id} ({pkg}): No patch links found\033[00m\n")
    # If no patch links for pkg affected by CVE
    return 0


def setup_parser():
    """
    Creates parser for arguments needed for get_uct_patch
    """
    argparser = argparse.ArgumentParser(
        description="Script for downloading CVE patches\n\nExample:\n\n\t./uct-get-patch --cves CVE-2025-1234 CVE-2024-2345 --pkg curl --dest /tmp --force",
        formatter_class=argparse.RawTextHelpFormatter,
    )
    argparser.add_argument(
        "--cves", type=str, nargs="*", help="CVEs to download patches for"
    )
    argparser.add_argument(
        "--dest", type=str, default=".", help="destination dir to download patches"
    )
    argparser.add_argument(
        "--pkg", type=str, required=True, help="filter patches by package"
    )
    argparser.add_argument("--uct-dir", type=str, help="UCT directory")
    argparser.add_argument(
        "--force",
        action="store_true",
        default=False,
        help="Save commit request when rate limited",
    )
    argparser.add_argument(
        "--silent", action="store_true", default=False, help="Silence output"
    )
    argparser.add_argument(
        "--timeout", type=int, default=10, help="Set timeout for requests in seconds"
    )
    return argparser


if __name__ == "__main__":
    parser = setup_parser()
    (opt, args) = parser.parse_known_args()

    if opt.pkg is None:
        parser.print_help()
        sys.exit(1)

    valid_cves: list[str] = []

    if opt.cves:
        for cve in opt.cves:
            if re.match(r"CVE-[0-9]{4}-[0-9]{4,7}", cve):
                valid_cves.append(cve)
                continue

            print(f"Ignoring {cve} as could not be recognized as a valid CVE ID")

    # Create patches directory or warn if it already exists
    patch_dir = Path(opt.dest) / "patches"

    if not patch_dir.is_dir():
        try:
            patch_dir.mkdir()
        except Exception:
            print("Couldn't create patches directory")
            sys.exit(1)
    else:
        print('"patches" directory already exists, storing patches here!')

    get_uct_patch(valid_cves, opt.pkg, patch_dir, opt.uct_dir, opt.force, opt.silent, opt.timeout)

Candidate: CVE-2026-21869
PublicDate: 2026-01-08 00:16:00 UTC
References:
 https://www.cve.org/CVERecord?id=CVE-2026-21869
 https://github.com/ggml-org/llama.cpp/security/advisories/GHSA-8947-pfff-2f3c
Description:
 llama.cpp is an inference of several LLM models in C/C++. In commits
 55d4206c8 and prior, the n_discard parameter is parsed directly from JSON
 input in the llama.cpp server's completion endpoints without validation to
 ensure it's non-negative. When a negative value is supplied and the context
 fills up, llama_memory_seq_rm/add receives a reversed range and negative
 offset, causing out-of-bounds memory writes in the token evaluation loop.
 This deterministic memory corruption can crash the process or enable remote
 code execution (RCE). There is no fix at the time of publication.
Ubuntu-Description:
Notes:
Mitigation:
Bugs:
 http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=1125060
Priority: medium
Discovered-by:
Assigned-to:
CVSS:
 github: CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H [8.8 HIGH]

Patches_llama.cpp:
upstream_llama.cpp: needs-triage
jammy_llama.cpp: DNE
noble_llama.cpp: DNE
plucky_llama.cpp: DNE
questing_llama.cpp: needs-triage
devel_llama.cpp: needs-triage

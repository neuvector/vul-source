Candidate: CVE-2025-53630
PublicDate: 2025-07-10 20:15:00 UTC
References:
 https://www.cve.org/CVERecord?id=CVE-2025-53630
 https://github.com/ggml-org/llama.cpp/security/advisories/GHSA-vgg9-87g3-85w8
 https://github.com/ggml-org/llama.cpp/commit/26a48ad699d50b6268900062661bd22f3e792579
Description:
 llama.cpp is an inference of several LLM models in C/C++. Integer Overflow
 in the gguf_init_from_file_impl function in ggml/src/gguf.cpp can lead to
 Heap Out-of-Bounds Read/Write. This vulnerability is fixed in commit
 26a48ad699d50b6268900062661bd22f3e792579.
Ubuntu-Description:
Notes:
Mitigation:
Bugs:
Priority: medium
Discovered-by:
Assigned-to:
CVSS:
 github: CVSS:4.0/AV:N/AC:L/AT:N/PR:N/UI:N/VC:H/VI:H/VA:H/SC:N/SI:N/SA:N/E:P [8.9 HIGH]

Patches_llama.cpp:
upstream_llama.cpp: needs-triage
jammy_llama.cpp: DNE
noble_llama.cpp: DNE
plucky_llama.cpp: DNE
questing_llama.cpp: needs-triage
devel_llama.cpp: needs-triage

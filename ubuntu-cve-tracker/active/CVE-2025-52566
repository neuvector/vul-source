Candidate: CVE-2025-52566
PublicDate: 2025-06-24 04:15:00 UTC
References:
 https://www.cve.org/CVERecord?id=CVE-2025-52566
 https://github.com/ggml-org/llama.cpp/security/advisories/GHSA-7rxv-5jhh-j6xx
 https://github.com/ggml-org/llama.cpp/commit/dd6e6d0b6a4bbe3ebfc931d1eb14db2f2b1d70af (b5721)
 https://github.com/ggml-org/llama.cpp/commit/dd6e6d0b6a4bbe3ebfc931d1eb14db2f2b1d70af
Description:
 llama.cpp is an inference of several LLM models in C/C++. Prior to version
 b5721, there is a signed vs. unsigned integer overflow in llama.cpp's
 tokenizer implementation (llama_vocab::tokenize) (src/llama-vocab.cpp:3036)
 resulting in unintended behavior in tokens copying size comparison.
 Allowing heap-overflowing llama.cpp inferencing engine with carefully
 manipulated text input during tokenization process. This issue has been
 patched in version b5721.
Ubuntu-Description:
Notes:
Mitigation:
Bugs:
Priority: medium
Discovered-by:
Assigned-to:
CVSS:
 github: CVSS:3.1/AV:L/AC:L/PR:N/UI:R/S:C/C:H/I:H/A:H [8.6 HIGH]
 nvd: CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H [8.8 HIGH]

Patches_llama.cpp:
upstream_llama.cpp: needs-triage
jammy_llama.cpp: DNE
noble_llama.cpp: DNE
oracular_llama.cpp: DNE
plucky_llama.cpp: DNE
devel_llama.cpp: needs-triage
